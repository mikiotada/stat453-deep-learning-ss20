{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 453: Deep Learning (Spring 2020)\n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)\n",
    "- Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2020/ \n",
    "- GitHub repository: https://github.com/rasbt/stat453-deep-learning-ss20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L24QVN8uwirR"
   },
   "source": [
    "A simple character-level RNN to generate new bits of text based on text from a novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moNmVfuvnImW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 7.9.0\n",
      "\n",
      "torch 1.4.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import time\n",
    "import random\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSRL42Qgy8I8"
   },
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvW1RgfepCBq"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TEXT_PORTION_SIZE = 200\n",
    "\n",
    "NUM_ITER = 20000\n",
    "LEARNING_RATE = 0.005\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "NUM_HIDDEN = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQMmKUEisW4W"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GnH64XvsV8n"
   },
   "source": [
    "Download *[A Tale of Two Cities](http://www.gutenberg.org/files/98/98-0.txt)* by Charles Dickens from the Gutenberg Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "WZ_4jiHVnMxN",
    "outputId": "40d01460-fe20-476b-c270-1fee964bcf9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-13 20:28:38--  http://www.gutenberg.org/files/98/98-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 804335 (785K) [text/plain]\n",
      "Saving to: ‘98-0.txt.1’\n",
      "\n",
      "98-0.txt.1          100%[===================>] 785.48K  2.20MB/s    in 0.3s    \n",
      "\n",
      "2020-04-13 20:28:38 (2.20 MB/s) - ‘98-0.txt.1’ saved [804335/804335]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.gutenberg.org/files/98/98-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-TBwKWPslPa"
   },
   "source": [
    "Convert all characters into ASCII characters provided by `string.printable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i8lk0nEnih-x",
    "outputId": "3c9c5cca-55a9-4e7e-a02b-64f8b9cea6c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e8uNrjdtn4A8",
    "outputId": "28b78ff2-8b68-4fe8-d01e-c27c8692f3ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text: 776058\n"
     ]
    }
   ],
   "source": [
    "with open('./98-0.txt', 'r') as f:\n",
    "    textfile = f.read()\n",
    "\n",
    "# convert special characters\n",
    "textfile = unidecode.unidecode(textfile)\n",
    "\n",
    "# strip extra whitespaces\n",
    "textfile = re.sub(' +',' ', textfile)\n",
    "\n",
    "TEXT_LENGTH = len(textfile)\n",
    "\n",
    "print(f'Number of characters in text: {TEXT_LENGTH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpEMNInXtZsb"
   },
   "source": [
    "Divide the text into smaller portions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "i7JiHR1stHNF",
    "outputId": "b99c6a1d-6906-4845-e9fe-20fad9ae9315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " left his saw sticking in the firewood he was cutting, set it in\n",
      "motion again; the women who had left on a door-step the little pot of\n",
      "hot ashes, at which she had been trying to soften the pain in her \n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "def random_portion(textfile):\n",
    "    start_index = random.randint(0, TEXT_LENGTH - TEXT_PORTION_SIZE)\n",
    "    end_index = start_index + TEXT_PORTION_SIZE + 1\n",
    "    return textfile[start_index:end_index]\n",
    "\n",
    "print(random_portion(textfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhZQCuBZkzgm"
   },
   "source": [
    "Define a function to convert characters into tensors of integers (type long):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ga-_g7Ltk479",
    "outputId": "2dd6fcd9-01b4-4910-f57a-812d05b9a4d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "def char_to_tensor(text):\n",
    "    lst = [string.printable.index(c) for c in text]\n",
    "    tensor = torch.tensor(lst).long()\n",
    "    return tensor\n",
    "\n",
    "print(char_to_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ph8OESgtmUqD"
   },
   "source": [
    "Putting it together to make a function that draws random batches for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_mVm5LOl95j"
   },
   "outputs": [],
   "source": [
    "def draw_random_sample(textfile):    \n",
    "    text_long = char_to_tensor(random_portion(textfile))\n",
    "    inputs = text_long[:-1]\n",
    "    targets = text_long[1:]\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "uY8FnmSkxGZ-",
    "outputId": "c332573b-5eb4-4367-bc4c-0a9a479aebb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([94, 17, 18, 28, 94, 32, 18, 23, 13, 24, 32, 94, 10, 28, 94, 18, 15, 94,\n",
       "         29, 17, 10, 29, 94, 32, 14, 27, 14, 94, 27, 30, 21, 14, 13, 94, 15, 24,\n",
       "         27, 94, 15, 18, 16, 30, 27, 14, 28, 94, 29, 24, 24, 73, 94, 10, 23, 13,\n",
       "         94, 14, 31, 14, 27, 34, 29, 17, 18, 23, 16, 96, 30, 23, 13, 14, 27, 94,\n",
       "         29, 17, 14, 94, 12, 21, 24, 30, 13, 28, 94, 32, 14, 27, 14, 94, 10, 94,\n",
       "         28, 30, 22, 75, 96, 96, 63, 43, 10, 21, 21, 24, 10, 62, 63, 94, 28, 10,\n",
       "         18, 13, 94, 48, 27, 75, 94, 54, 29, 27, 34, 31, 14, 27, 75, 94, 63, 43,\n",
       "         24, 32, 94, 13, 24, 94, 34, 24, 30, 94, 13, 24, 82, 94, 44, 94, 17, 24,\n",
       "         25, 14, 94, 34, 24, 30, 94, 10, 27, 14, 94, 32, 14, 21, 21, 62, 63, 96,\n",
       "         96, 44, 29, 94, 32, 10, 28, 94, 54, 29, 27, 34, 31, 14, 27, 68, 28, 94,\n",
       "         16, 27, 10, 23, 13, 94, 25, 14, 12, 30, 21, 18, 10, 27, 18, 29, 34, 94,\n",
       "         29, 17]),\n",
       " tensor([17, 18, 28, 94, 32, 18, 23, 13, 24, 32, 94, 10, 28, 94, 18, 15, 94, 29,\n",
       "         17, 10, 29, 94, 32, 14, 27, 14, 94, 27, 30, 21, 14, 13, 94, 15, 24, 27,\n",
       "         94, 15, 18, 16, 30, 27, 14, 28, 94, 29, 24, 24, 73, 94, 10, 23, 13, 94,\n",
       "         14, 31, 14, 27, 34, 29, 17, 18, 23, 16, 96, 30, 23, 13, 14, 27, 94, 29,\n",
       "         17, 14, 94, 12, 21, 24, 30, 13, 28, 94, 32, 14, 27, 14, 94, 10, 94, 28,\n",
       "         30, 22, 75, 96, 96, 63, 43, 10, 21, 21, 24, 10, 62, 63, 94, 28, 10, 18,\n",
       "         13, 94, 48, 27, 75, 94, 54, 29, 27, 34, 31, 14, 27, 75, 94, 63, 43, 24,\n",
       "         32, 94, 13, 24, 94, 34, 24, 30, 94, 13, 24, 82, 94, 44, 94, 17, 24, 25,\n",
       "         14, 94, 34, 24, 30, 94, 10, 27, 14, 94, 32, 14, 21, 21, 62, 63, 96, 96,\n",
       "         44, 29, 94, 32, 10, 28, 94, 54, 29, 27, 34, 31, 14, 27, 68, 28, 94, 16,\n",
       "         27, 10, 23, 13, 94, 25, 14, 12, 30, 21, 18, 10, 27, 18, 29, 34, 94, 29,\n",
       "         17, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_random_sample(textfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_grdW3pxCzz"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQIUm5EjxFNa"
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, embed_size,\n",
    "                 hidden_size, output_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(input_size=embed_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=num_layers)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.init_hidden = torch.nn.Parameter(torch.zeros(\n",
    "                                              num_layers, 1, hidden_size))\n",
    "    \n",
    "    def forward(self, features, hidden):\n",
    "        embedded = self.embed(features.view(1, -1))\n",
    "        output, hidden = self.gru(embedded.view(1, 1, -1), hidden)\n",
    "        output = self.fc(output.view(1, -1))\n",
    "        return output, hidden\n",
    "      \n",
    "    def init_zero_state(self):\n",
    "        init_hidden = torch.zeros(self.num_layers, 1, self.hidden_size).to(DEVICE)\n",
    "        return init_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik3NF3faxFmZ"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(len(string.printable), EMBEDDING_DIM, HIDDEN_DIM, len(string.printable), NUM_HIDDEN)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lv9Ny9di6VcI"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5XlCqm3pMj2"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    ## based on https://github.com/spro/practical-pytorch/\n",
    "    ## blob/master/char-rnn-generation/char-rnn-generation.ipynb\n",
    "\n",
    "    hidden = model.init_zero_state()\n",
    "    prime_input = char_to_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[p].to(DEVICE), hidden.to(DEVICE))\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp.to(DEVICE), hidden.to(DEVICE))\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = string.printable[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_to_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3536
    },
    "colab_type": "code",
    "id": "ZNv7_wE4prjX",
    "outputId": "8ae1c36b-e4e8-4fb6-852d-ac926ad34cb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.00 min\n",
      "Iteration 0 | Loss 4.58\n",
      "\n",
      "\n",
      "Th6#hb:B2iKt0v>E$siC?Q./gyt'y9 o- V ?YA!@hpO%#nsqn&rq1* D!u-*opw;bCc,ut-DAWh)cK&~+\f",
      "}10LOVG/GIpm64-)(m?,d~3kn?,9Hj8\tRmC?\n",
      "It er%qU?0) bE\tnh7 A7i$K\\UIc;#RJvcY+_i=r9KS>\tv|jf<#y2lz`CCWbDW6W\u000b",
      "_#_>FzE,{Jbje@ebq \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 3.58 min\n",
      "Iteration 1000 | Loss 1.84\n",
      "\n",
      "\n",
      "Th\n",
      "to old. He crour and pert were let of highers brive reall facher, his from ageally bead fore he knewure of or the brown as the shich from of the\n",
      "go her ty his fross treidef that in at projed to that  \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 7.15 min\n",
      "Iteration 2000 | Loss 1.78\n",
      "\n",
      "\n",
      "Then the gradadged yett wimed, and ence him uposs, and were stare her man wanted evither worked way what\n",
      "inter appation very. Stow man of the where who have to the bewchoway defured Defarge far prespera \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 10.70 min\n",
      "Iteration 3000 | Loss 1.74\n",
      "\n",
      "\n",
      "Thy at the mity his\n",
      "before it was he and and pertioned -athination was sile to be reverest\n",
      "knew this ig1.\"\n",
      "\n",
      "\"While Bard were fam him your peen in the countent it seat thousted, \"Then\n",
      "not to him, Doctry. \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 14.42 min\n",
      "Iteration 4000 | Loss 1.65\n",
      "\n",
      "\n",
      "Thress would stal have\n",
      "she was a stopped noting are is spock be where I an a see\n",
      "faid he poods alon was the eimely the other alone-red in\n",
      "hessed. He anowt hemore, caulder be beng his spoke be to\n",
      "trigrss \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 18.06 min\n",
      "Iteration 5000 | Loss 1.46\n",
      "\n",
      "\n",
      "The poor about it sight into sure the live\n",
      "are you, in alticuld before and an a Farate, one anwarms, which he wain that the\n",
      "when I mades in this close much alway.\"\n",
      "\n",
      "\"You time. \"I do. I ped the manniture \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 21.67 min\n",
      "Iteration 6000 | Loss 1.56\n",
      "\n",
      "\n",
      "Thene That he had\n",
      "see you, questing, and mer you more; for her herey said he is in the papitently. And which seen mer\n",
      "interested an his not humered, and in the whose not that\n",
      "which as more ertting upon  \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 25.28 min\n",
      "Iteration 7000 | Loss 1.67\n",
      "\n",
      "\n",
      "Ther of pray of will she he\n",
      "left egation of her left of among of her few with soon hearily asked\n",
      "his into his can moment at the Saind Are men more\n",
      "at a good in if the strogs past lost men for you my fel \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 28.86 min\n",
      "Iteration 8000 | Loss 1.66\n",
      "\n",
      "\n",
      "Thour you, where so is when\n",
      "the bacbilse vengered. And beam one chapy stupponed that was the hand\n",
      "were head with that wagened and in She don't had came that cleared were when the changle and days of tha \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 32.42 min\n",
      "Iteration 9000 | Loss 1.52\n",
      "\n",
      "\n",
      "The shope he rose whisten of stremist storing and to be been aslesentation to at and were he had said with\n",
      "his way of house, shood, to the sent show, and a a dissent his face has from the had it of hard \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 36.01 min\n",
      "Iteration 10000 | Loss 1.30\n",
      "\n",
      "\n",
      "Thend Gutenberg's was are her said of thing shake which\n",
      "had of mine who loiness. He work, and off through his said as he she standweer his was thoass man ille\n",
      "good to with a firms who worked and whom hi \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 39.59 min\n",
      "Iteration 11000 | Loss 1.81\n",
      "\n",
      "\n",
      "Th arm.\n",
      "\n",
      "\"Indety. But, he saughe of the with the\n",
      "preses expricidinges of child. The looking at the with hard her down a reservation to\n",
      "a did strieur was a stailed get on the monsimighat deswerement the  \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 43.18 min\n",
      "Iteration 12000 | Loss 1.64\n",
      "\n",
      "\n",
      "The lide to sather. The law being with her\n",
      "her follow to a crusted me the insung her house than, and never her\n",
      "hear and spoluined to Tenferer not himself of neeching to the strong the\n",
      "work the pried tha \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 46.76 min\n",
      "Iteration 13000 | Loss 1.47\n",
      "\n",
      "\n",
      "This freath of time of first\n",
      "made fratent, were us taken maury of the black with abemens of looked, the life\n",
      "oblish were fixed and other they digs, one of the opened immers, and outtion of the mane a\n",
      "me \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 50.37 min\n",
      "Iteration 14000 | Loss 1.82\n",
      "\n",
      "\n",
      "The eyess, benain and an owating this\n",
      "enchingrs of the proppated, in the mered then it a devishter, gentle of the can the works trose of the goldered at it read paid of ate of him there was near\n",
      "one obs \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 53.97 min\n",
      "Iteration 15000 | Loss 1.52\n",
      "\n",
      "\n",
      "The more on thee to-dessions;\n",
      "as hen hood certoring assomes, had order the what seemest more the wife\n",
      "no woded, there all try, and must this way she care of ancter way.\n",
      "\n",
      "\"Hustiended any shood as there,  \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 57.53 min\n",
      "Iteration 16000 | Loss 1.54\n",
      "\n",
      "\n",
      "The Knith nain and darf,\n",
      "whimsered-staction rath the fellow, and down of not be not dagure. The good on 457e a faces and hung ungere when the came and\n",
      "from a the blame was overing of his and heraring wi \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 61.11 min\n",
      "Iteration 17000 | Loss 1.55\n",
      "\n",
      "\n",
      "Then, and since, that\n",
      "imprsider an more by the Frange roold feet more, and there gone wank a little a comrong to a so bous the\n",
      "prisore it was to myself, they so means of which a what intere the brown.\n",
      "\n",
      " \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 64.67 min\n",
      "Iteration 18000 | Loss 1.56\n",
      "\n",
      "\n",
      "The say close,\n",
      "and the danger folded there down from his a the danking at her..\n",
      "\n",
      "\"It have have as atton to being and happle and forfartuen way, them gonged\n",
      "the baker's shore they good and Brongth with a \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 68.27 min\n",
      "Iteration 19000 | Loss 1.77\n",
      "\n",
      "\n",
      "Thew by a like a\n",
      "Dan\n",
      "\n",
      "\n",
      "By remangere a surpprison.\n",
      "\n",
      "\"A though preve interved suction of the longed inting, Judn'tently\n",
      "eith--\"Yon' she was, disely of itt-his had been provant to door time at the\n",
      "daughter \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for iteration in range(NUM_ITER):\n",
    "\n",
    "    \n",
    "    ### FORWARD AND BACK PROP\n",
    "\n",
    "    hidden = model.init_zero_state()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0.\n",
    "    inputs, targets = draw_random_sample(textfile)\n",
    "    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "    for c in range(TEXT_PORTION_SIZE):\n",
    "        outputs, hidden = model(inputs[c], hidden)\n",
    "        loss += F.cross_entropy(outputs, targets[c].view(1))\n",
    "\n",
    "    loss /= TEXT_PORTION_SIZE\n",
    "    loss.backward()\n",
    "    \n",
    "    ### UPDATE MODEL PARAMETERS\n",
    "    optimizer.step()\n",
    "\n",
    "    ### LOGGING\n",
    "    with torch.set_grad_enabled(False):\n",
    "      if iteration % 1000 == 0:\n",
    "          print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "          print(f'Iteration {iteration} | Loss {loss.item():.2f}\\n\\n')\n",
    "          print(evaluate(model, 'Th', 200), '\\n')\n",
    "          print(50*'=')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "char-rnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
